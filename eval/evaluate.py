import argparse
import datetime
import logging
import subprocess
from pathlib import Path

import pandas as pd
from rich.logging import RichHandler

from eval.transformer_model import TransformerModel


class FranklinEvaluator:
    """Evaluate the performance of a transformer model on a split/portion/template of the dataset."""

    def __init__(
        self,
        model_name: str,
        dataset: pd.DataFrame,
        use_tools: str = 'full',
        debug: bool = False,
        save: bool = False,
        description: str = '',
    ):
        """Initialize the evaluator.

        Parameters
        ----------
        model_name : str
            Name of the transformer model.
        dataset : pd.DataFrame
            The inputs to be used for evaluation. This should be a dataset file from `datasets/`.
        use_tools : str
            Tool-use mode. Can be 'full', 'simulate', or 'none'.
        debug : bool
            Enable debug mode for the model.
        save : bool
            Whether to save the results or not.
        description : str
            Description of the evaluation.

        """
        self.model = TransformerModel(
            model_name=model_name,
            use_tools=use_tools,
            debug=debug,
        )
        self.dataset = dataset
        self.use_tools = use_tools
        self.save = save
        self.description = description

    def run(self) -> dict:
        """Evaluate the model on the dataset.

        Returns
        -------
        dict
            The full list of messages generated by the model.

        """
        # Get list of inputs from question column
        # inputs = self.dataset["question"]
        all_messages = []

        # Filename is in format 2025-01-27T15:45:03.jsonl
        timestamp = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')

        # Generate messages for each input
        for i, row in self.dataset.iterrows():
            logging.info(f'Processing question {i + 1}/{len(self.dataset)}')
            messages = self.model.loop(row['question'], max_loops=row['metadata']['total_actions'] * 10)

            # Extract the content of the 'final_answer' tool call
            final_answer = next(
                (
                    message['content']
                    for message in messages
                    if message.get('role') == 'tool' and message.get('name') == 'final_answer'
                ),
                None,
            )

            # Log the comparison between the final answer and the expected answer
            if final_answer is not None:
                logging.info(f'Final answer: {final_answer}')
                logging.info(f'Expected answer: {row["answer"]}')
                if str(final_answer) == str(row['answer']):
                    logging.info('✅ Final answer matches the expected answer.')
                else:
                    logging.warning('❌ Final answer does not match the expected answer.')
            else:
                logging.warning('⚠️ No final answer found in the messages.')

            all_messages.append(messages)

        self.dataset['messages'] = all_messages

        if self.save:
            output_path = Path('eval', 'runs', f'{timestamp}.jsonl')
            output_path.parent.mkdir(parents=True, exist_ok=True)
            self.dataset.to_json(output_path, orient='records', lines=True)
            logging.info(f'Saved evaluation results to {output_path}')

        return all_messages


if __name__ == '__main__':
    FORMAT = '%(message)s'
    logging.basicConfig(
        level=logging.INFO,
        format=FORMAT,
        datefmt='[%X]',
        handlers=[RichHandler()],
    )

    parser = argparse.ArgumentParser(description='Evaluate a transformer model.')
    parser.add_argument(
        '--model_name',
        type=str,
        default='/public/hf/models/meta-llama/Meta-Llama-3.1-8B-Instruct/',
        help='Path or name of the transformer model.',
    )
    parser.add_argument(
        '--template',
        type=str,
        default='PropertyOfSubject',
        help='Path to the dataset file.',
    )
    parser.add_argument(
        '--split',
        type=str,
        default='answerable',
        help='Split of the dataset to use for evaluation.',
    )
    parser.add_argument(
        '--num_samples',
        type=int,
        default=20,
        help='Number of samples to evaluate.',
    )
    parser.add_argument(
        '--use_tools',
        type=str,
        choices=['full', 'simulate', 'none'],
        default='full',
        help="Tool-use mode. Can be 'full', 'simulate', or 'none'.",
    )
    parser.add_argument(
        '--save',
        action='store_true',
        help='Whether to save the evaluation results.',
    )
    parser.add_argument(
        '--description',
        type=str,
        default='',
        help='Description of the evaluation.',
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Enable debug mode for the model.',
    )
    parser.add_argument(
        '--load_from_batch',
        action='store_true',
        help='Load configurations from a batch file.',
    )
    args = parser.parse_args()

    if args.load_from_batch:
        # Load batch configurations from the file
        batch_configs = pd.read_json(Path('eval', 'batch.jsonl'), orient='records', lines=True)

        logging.info(f'Loaded {len(batch_configs)} configurations from batch.jsonl')

        subprocess.run(['vllm', 'serve', batch_configs[0]['model_name']], check=False)

        for config in batch_configs.to_dict(orient='records'):
            logging.info('Running evaluation for the following configuration:')
            logging.info(config)

            # Load dataset for the current configuration
            dataset = pd.read_json(
                Path('dataset', config['split'], config['template']).with_suffix('.jsonl'),
                orient='records',
                lines=True,
            ).sample(config['num_samples'])

            # Initialize evaluator
            evaluator = FranklinEvaluator(
                model_name=config['model_name'],
                dataset=dataset,
                use_tools=config['use_tools'],
                debug=config['debug'],
                save=config['save'],
                description=config['description'],
            )

            # Run evaluation
            evaluator.run()
    else:
        # Single configuration mode
        dataset = pd.read_json(
            Path('dataset', args.split, args.template).with_suffix('.jsonl'),
            orient='records',
            lines=True,
        ).sample(args.num_samples)

        evaluator = FranklinEvaluator(
            model_name=args.model_name,
            dataset=dataset,
            use_tools=args.use_tools,
            debug=args.debug,
            save=args.save,
            description=args.description,
        )

        logging.info('Launching evaluation with the following configuration:')
        logging.info(f'Model name: {args.model_name}')
        logging.info(f'Template: {args.template}')
        logging.info(f'Split: {args.split}')
        logging.info(f'Number of samples: {args.num_samples}')
        logging.info(f'Use tools: {args.use_tools}')
        logging.info(f'Save results: {args.save}')
        logging.info(f'Description: {args.description}')
        logging.info(f'Debug mode: {args.debug}')
        logging.info(f'Dataset size: {len(dataset)}')

        evaluator.run()
