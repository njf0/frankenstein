import argparse
import logging
from pathlib import Path

import openai
import pandas as pd
from rich.logging import RichHandler

from eval.runner import Runner  # <-- Use Runner instead of vLLMModel


class FrankensteinEvaluator:
    """Evaluate the performance of a transformer model on a split/portion/template of the dataset."""

    def __init__(
        self,
        model_name: str,
        toolbox: str = 'all',
        save: bool = False,
        num_samples: int = -1,
        split: str = 'answerable-full',
        n_shots: int = 0,
    ):
        """Initialize the evaluator.

        Parameters
        ----------
        model_name : str
            Path or name of the transformer model.
        toolbox : str
            Toolbox to use for the evaluation. Can be 'all', 'arithmetic', 'data', or 'none'.
        save : bool
            Whether to save the evaluation results.
        num_samples : int
            Number of samples to evaluate.
        split : str
            Dataset split to use.
        n_shots : int
            Number of n-shot tool call examples to prepend to the prompt.

        """
        self.model_name = model_name
        self.toolbox = toolbox
        self.save = save
        self.num_samples = num_samples
        self.split = split
        self.n_shots = n_shots

        # Load dataset from dataset/{split}.jsonl or .json
        dataset_path = Path('dataset', f'{self.split}.jsonl')
        self.dataset = pd.read_json(dataset_path, orient='records', lines=True)
        if self.num_samples != -1:
            self.dataset = self.dataset.sample(self.num_samples)
        logging.info(f'Loaded dataset from {dataset_path} with {len(self.dataset)} samples.')

        self.log_config(vars(self))

    def run(
        self,
    ) -> list:
        """Evaluate the model on the dataset.

        Returns
        -------
        list
            List of messages generated by the model for each input in the dataset.

        """
        all_messages = []
        corrects = []
        errors = []
        i = 1

        for _, row in self.dataset.iterrows():
            self.runner = Runner(
                model_name=self.model_name,
                toolbox=self.toolbox,
                n_shots=self.n_shots,
            )

            logging.info(f'✨ Processing question {i}/{len(self.dataset)}')
            # Should log slot values to compare with the model's output

            logging.info('Question Metadata')
            self.log_metadata_table(row['metadata'])

            messages = self.runner.loop(row['question'])

            # Extract gold answer and answer_format
            gold_answer = row['answer']
            answer_format = None
            if isinstance(row, pd.Series) and 'metadata' in row and isinstance(row['metadata'], dict):
                answer_format = row['metadata'].get('answer_format')

            # Use Runner for evaluation and logging
            match_result = self.runner.match_results(messages, gold_answer, answer_format)
            if match_result is not None:
                correct, error = match_result
            else:
                correct, error = False, 100.0
            corrects.append(correct)
            errors.append(error)

            all_messages.append(self.runner.format_messages(messages))

            i += 1

        self.dataset['messages'] = all_messages
        self.dataset['correct'] = corrects
        self.dataset['error'] = errors

        if self.save:
            model_name = str(self.model_name).split('/')[-1]
            output_path = Path('eval', 'runs', f'{model_name}_{self.split}.jsonl')
            output_path.parent.mkdir(parents=True, exist_ok=True)
            self.dataset.to_json(output_path, orient='records', lines=True)
            logging.info(f'Saved evaluation results to {output_path}')

        return all_messages

    def log_config(
        self,
        config: dict,
    ) -> None:
        """Log the configuration in a formatted way.

        Parameters
        ----------
        config : dict
            Configuration dictionary to log.

        """
        key_width = max(len(str(k)) for k in config)
        logging.info('Model Config')
        for k, v in config.items():
            if k == 'dataset':
                continue
            arrow = '-' * (key_width + 1 - len(str(k))) + '>'
            logging.info(f"⚙️ '{k}' {arrow} {v!r}")

    def log_metadata_table(
        self,
        metadata: dict,
    ) -> None:
        """Log metadata in a formatted table.

        Parameters
        ----------
        metadata : dict
            Metadata dictionary to log.

        """
        key_width = max(len(str(k)) for k in metadata)
        for k, v in metadata.items():
            if k == 'slot_values':
                self.log_metadata_table(v)
            else:
                # Arrow line replaces padding: key + ('-' * (key_width - len(key))) + '>'
                arrow = '-' * (key_width + 1 - len(str(k))) + '>'
                # Can we add a good emoji for 'metadata'?
                logging.info(f"🔑 '{k}' {arrow} {v!r}")


if __name__ == '__main__':
    FORMAT = '%(message)s'
    logging.basicConfig(
        level=logging.INFO,
        format=FORMAT,
        datefmt='[%X]',
        handlers=[RichHandler(tracebacks_suppress=[openai])],
    )

    parser = argparse.ArgumentParser(description='Evaluate a transformer model.')
    parser.add_argument(
        '--model-name',
        type=str,
        default='/public/hf/models/meta-llama/Meta-Llama-3.1-8B-Instruct/',
        help='Path or name of the transformer model.',
    )
    parser.add_argument(
        '--split',
        type=str,
        default='answerable-full',
        help='Dataset split to use (e.g., "answerable-full", "unanswerable-partial", etc.).',
    )
    parser.add_argument(
        '--num-samples',
        type=int,
        default=-1,
        help='Number of samples to evaluate. Use -1 for all samples.',
    )
    parser.add_argument(
        '--toolbox',
        type=str,
        choices=['all', 'arithmetic', 'data', 'none'],
        default='all',
        help='Toolbox to use for the evaluation. Can be "all", "arithmetic", "data", or "none".',
    )
    parser.add_argument(
        '--save',
        action='store_true',
        help='Whether to save the evaluation results.',
    )
    parser.add_argument(
        '--n-shots',
        type=int,
        default=0,
        help='Number of n-shot tool call examples to prepend to the prompt.',
    )
    args = parser.parse_args()

    evaluator = FrankensteinEvaluator(
        model_name=args.model_name,
        toolbox=args.toolbox,
        save=args.save,
        num_samples=args.num_samples,
        split=args.split,
        n_shots=args.n_shots,
    )
    evaluator.args = args  # Attach args for logging

    evaluator.run()
