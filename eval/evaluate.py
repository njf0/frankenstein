import argparse
import datetime
import gc  # Add this import at the top
import json
import logging
from pathlib import Path

import pandas as pd
from rich.logging import RichHandler

from eval.generate import vLLMModel


class FranklinEvaluator:
    """Evaluate the performance of a transformer model on a split/portion/template of the dataset."""

    def __init__(self, **kwargs):
        """Initialize the evaluator.

        Parameters
        ----------
        kwargs : dict
            All arguments passed to the parser, except `load_from_batch`.

        """
        self.model_name = kwargs.get('model_name')
        self.use_tools = kwargs.get('use_tools', 'full')
        self.debug = kwargs.get('debug', False)
        self.save = kwargs.get('save', False)
        self.description = kwargs.get('description', '')
        self.num_samples = kwargs.get('num_samples', 20)
        self.template = kwargs.get('template', '')
        self.split = kwargs.get('split', '')
        self.n_shots = kwargs.get('n_shots', 0)

        # Load dataset
        dataset_path = Path('dataset', self.split, self.template).with_suffix('.jsonl')
        self.dataset = pd.read_json(dataset_path, orient='records', lines=True).head(self.num_samples)
        logging.info(f'Loaded dataset from {dataset_path} with {len(self.dataset)} samples.')

        self.model = vLLMModel(
            model_name=self.model_name,
            use_tools=self.use_tools,
            debug=self.debug,
            n_shots=self.n_shots,
        )

    def run(self) -> dict:
        """Evaluate the model on the dataset.

        Returns
        -------
        dict
            The full list of messages generated by the model.

        """
        # Get list of inputs from question column
        # inputs = self.dataset["question"]
        all_messages = []

        # Filename is in format 2025-01-27T15:45:03.jsonl
        timestamp = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')

        # Generate messages for each input
        for i, row in self.dataset.iterrows():
            logging.info(f'Processing question {i + 1}/{len(self.dataset)}')
            messages = self.model.loop(row['question'], max_loops=row['metadata']['total_actions'] * 10)

            # Extract the content of the 'final_answer' tool call
            final_answer = next(
                (
                    message['content']
                    for message in messages
                    if message.get('role') == 'tool' and message.get('name') == 'final_answer'
                ),
                None,
            )

            # Log the comparison between the final answer and the expected answer
            if final_answer is not None:
                logging.info(f'Final answer: {final_answer}')
                logging.info(f'Expected answer: {row["answer"]}')
                if str(final_answer) == str(row['answer']) or str(row['answer']) in str(final_answer):
                    logging.info('✅ Final answer matches the expected answer.')
                else:
                    logging.warning('❌ Final answer does not match the expected answer.')
            else:
                logging.warning('⚠️ No final answer found in the messages.')

            all_messages.append(messages)

        self.dataset['messages'] = all_messages

        if self.save:
            output_path = Path('eval', 'runs', f'{timestamp}.jsonl')
            output_path.parent.mkdir(parents=True, exist_ok=True)
            self.dataset.to_json(output_path, orient='records', lines=True)
            logging.info(f'Saved evaluation results to {output_path}')

            # Log the configuration and filename to log.jsonl
            log_entry = {
                'model_name': self.model_name,
                'split': self.split,
                'num_samples': self.num_samples,
                'use_tools': self.use_tools,
                'save': self.save,
                'description': self.description,
                'debug': self.debug,
                'template': self.template,
                'filename': str(output_path),
                'timestamp': timestamp,
            }
            log_path = Path('eval', 'log.jsonl')
            with log_path.open('a') as log_file:
                log_file.write(json.dumps(log_entry) + '\n')
            logging.info(f'Logged evaluation configuration to {log_path}')

        return all_messages


if __name__ == '__main__':
    FORMAT = '%(message)s'
    logging.basicConfig(
        level=logging.INFO,
        format=FORMAT,
        datefmt='[%X]',
        handlers=[RichHandler()],
    )

    parser = argparse.ArgumentParser(description='Evaluate a transformer model.')
    parser.add_argument(
        '--model_name',
        type=str,
        default='/public/hf/models/meta-llama/Meta-Llama-3.1-8B-Instruct/',
        help='Path or name of the transformer model.',
    )
    parser.add_argument(
        '--template',
        type=str,
        default='PropertyOfSubject',
        help='Path to the dataset file.',
    )
    parser.add_argument(
        '--split',
        type=str,
        default='answerable',
        help='Split of the dataset to use for evaluation.',
    )
    parser.add_argument(
        '--num_samples',
        type=int,
        default=20,
        help='Number of samples to evaluate.',
    )
    parser.add_argument(
        '--use_tools',
        type=str,
        choices=['full', 'simulate', 'none'],
        default='full',
        help="Tool-use mode. Can be 'full', 'simulate', or 'none'.",
    )
    parser.add_argument(
        '--save',
        action='store_true',
        help='Whether to save the evaluation results.',
    )
    parser.add_argument(
        '--description',
        type=str,
        default='',
        help='Description of the evaluation.',
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Enable debug mode for the model.',
    )
    parser.add_argument(
        '--n_shots',
        type=int,
        default=0,
        help='Number of n-shot tool call examples to prepend to the prompt.',
    )
    parser.add_argument(
        '--load_from_batch',
        action='store_true',
        help='Load configurations from a batch file.',
    )
    args = parser.parse_args()

    if args.load_from_batch:
        # Load batch configurations from the file
        batch_configs = pd.read_json(Path('eval', 'batch.jsonl'), orient='records', lines=True)

        logging.info(f'Loaded {len(batch_configs)} configurations from batch.jsonl')

        for i, config in enumerate(batch_configs.to_dict(orient='records')):
            logging.info(f'Running configuration {i + 1}/{len(batch_configs)}:')
            logging.info(config)

            # Initialize evaluator
            evaluator = FranklinEvaluator(**config)

            # Run evaluation
            evaluator.run()

            # Unload the model and free memory
            del evaluator  # Delete the evaluator instance
            gc.collect()  # Force garbage collection

    else:
        # Single configuration mode
        evaluator = FranklinEvaluator(
            model_name=args.model_name,
            use_tools=args.use_tools,
            debug=args.debug,
            save=args.save,
            description=args.description,
            num_samples=args.num_samples,
            template=args.template,
            split=args.split,
            n_shots=args.n_shots,
        )

        logging.info('Running config:')
        logging.info(vars(args))

        evaluator.run()

        del evaluator
        gc.collect()
