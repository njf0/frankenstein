import argparse
import json
import logging
from pathlib import Path

import pandas as pd
from rich.logging import RichHandler

from eval.matcher import Matcher  # <-- Import Matcher from new file
from eval.runner import Runner  # <-- Use Runner instead of vLLMModel


class FrankensteinEvaluator:
    """Evaluate the performance of a transformer model on a split/portion/template of the dataset."""

    def __init__(
        self,
        model_name: str,
        toolbox: str = 'all',
        save: bool = False,
        num_samples: int = -1,
        split: str = 'answerable-full',
        n_shots: int = 0,
    ):
        """Initialize the evaluator.

        Parameters
        ----------
        model_name : str
            Path or name of the transformer model.
        toolbox : str
            Toolbox to use for the evaluation. Can be 'all', 'arithmetic', 'data', or 'none'.
        save : bool
            Whether to save the evaluation results.
        num_samples : int
            Number of samples to evaluate.
        split : str
            Dataset split to use.
        n_shots : int
            Number of n-shot tool call examples to prepend to the prompt.

        """
        self.model_name = model_name
        self.toolbox = toolbox
        self.save = save
        self.num_samples = num_samples
        self.split = split
        self.n_shots = n_shots

        # Load dataset from dataset/{split}.jsonl or .json
        dataset_path = Path('dataset', f'{self.split}.jsonl')
        self.dataset = pd.read_json(dataset_path, orient='records', lines=True)
        if self.num_samples != -1:
            self.dataset = self.dataset.sample(self.num_samples)
        logging.info(f'Loaded dataset from {dataset_path} with {len(self.dataset)} samples.')

        self.matcher = Matcher()

    def run(
        self,
    ) -> list:
        """Evaluate the model on the dataset.

        Returns
        -------
        list
            List of messages generated by the model for each input in the dataset.

        """
        all_messages = []

        for i, row in self.dataset.iterrows():
            self.runner = Runner(
                model_name=self.model_name,
                toolbox=self.toolbox,
                n_shots=self.n_shots,
            )

            logging.info(f'Processing question {i + 1}/{len(self.dataset)}')
            messages = self.runner.loop(row['question'])

            # Extract the final answer using the same logic as runner.py
            final_answer = None
            for message in messages:
                if message.get('role') == 'assistant' and message.get('tool_calls'):
                    for tool_call in message['tool_calls']:
                        if tool_call.get('function', {}).get('name') == 'final_answer':
                            parsed_args = json.loads(tool_call['function']['arguments'])
                            final_answer = parsed_args.get('answer')
                            break
                if final_answer is not None:
                    break

            answer_format = None
            if isinstance(row, pd.Series) and 'metadata' in row and isinstance(row['metadata'], dict):
                answer_format = row['metadata'].get('answer_format')

            # Use Matcher for evaluation and log details
            if final_answer is not None:
                match_result = self.matcher.match(final_answer, row['answer'], answer_format)
            else:
                logging.warning('⚠️ No final answer found in the messages.')

            all_messages.append(self.runner.format_messages(messages))

        self.dataset['messages'] = all_messages

        if self.save:
            model_name = str(self.model_name).split('/')[-1]
            output_path = Path('eval', 'runs', f'{model_name}_{self.split}.jsonl')
            output_path.parent.mkdir(parents=True, exist_ok=True)
            self.dataset.to_json(output_path, orient='records', lines=True)
            logging.info(f'Saved evaluation results to {output_path}')

        return all_messages


if __name__ == '__main__':
    FORMAT = '%(message)s'
    logging.basicConfig(
        level=logging.INFO,
        format=FORMAT,
        datefmt='[%X]',
        handlers=[RichHandler()],
    )

    parser = argparse.ArgumentParser(description='Evaluate a transformer model.')
    parser.add_argument(
        '--model-name',
        type=str,
        default='/public/hf/models/meta-llama/Meta-Llama-3.1-8B-Instruct/',
        help='Path or name of the transformer model.',
    )
    parser.add_argument(
        '--split',
        type=str,
        default='answerable-full',
        help='Dataset split to use (e.g., "answerable-full", "unanswerable-partial", etc.).',
    )
    parser.add_argument(
        '--num-samples',
        type=int,
        default=-1,
        help='Number of samples to evaluate. Use -1 for all samples.',
    )
    parser.add_argument(
        '--toolbox',
        type=str,
        choices=['all', 'arithmetic', 'data', 'none'],
        default='all',
        help='Toolbox to use for the evaluation. Can be "all", "arithmetic", "data", or "none".',
    )
    parser.add_argument(
        '--save',
        action='store_true',
        help='Whether to save the evaluation results.',
    )
    parser.add_argument(
        '--n-shots',
        type=int,
        default=0,
        help='Number of n-shot tool call examples to prepend to the prompt.',
    )
    args = parser.parse_args()

    # Example usage
    # python eval/evaluate.py --model_name openai/gpt-4o-mini --split answerable-full --num_samples 10 --toolbox all --save --n_shots 0

    evaluator = FrankensteinEvaluator(
        model_name=args.model_name,
        toolbox=args.toolbox,
        save=args.save,
        num_samples=args.num_samples,
        split=args.split,
        n_shots=args.n_shots,
    )

    logging.info('Running config:')
    logging.info(vars(args))

    evaluator.run()
