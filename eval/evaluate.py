import argparse
import logging
from pathlib import Path

import openai
import pandas as pd
from rich.logging import RichHandler
from runner import Runner

FORMAT = '%(message)s'
logging.basicConfig(
    level=logging.INFO,
    format=FORMAT,
    datefmt='[%X]',
    handlers=[
        RichHandler(
            tracebacks_suppress=[openai],
            markup=True,
        )
    ],
)


class FrankensteinEvaluator:
    """Evaluate the performance of a transformer model on a split/portion/template of the dataset."""

    def __init__(
        self,
        model_name: str,
        toolbox: str = 'all',
        save: bool = False,
        num_samples: int = -1,
        split: str = 'answerable-full',
        n_shots: int = 0,
        debug: bool = False,  # Add debug argument
    ):
        """Initialize the evaluator.

        Parameters
        ----------
        model_name : str
            Path or name of the transformer model.
        toolbox : str
            Toolbox to use for the evaluation. Can be 'all', 'arithmetic', 'data', or 'none'.
        save : bool
            Whether to save the evaluation results.
        num_samples : int
            Number of samples to evaluate.
        split : str
            Dataset split to use.
        n_shots : int
            Number of n-shot tool call examples to prepend to the prompt.

        """
        self.model_name = model_name
        self.toolbox = toolbox
        self.save = save
        self.num_samples = num_samples
        self.split = split
        self.n_shots = n_shots
        self.debug = debug

        # Load dataset from dataset/{split}.jsonl or .json
        dataset_path = Path('dataset', f'{self.split}.jsonl')
        self.dataset = pd.read_json(dataset_path, orient='records', lines=True)
        if self.num_samples != -1:
            self.dataset = self.dataset.sample(self.num_samples)
        logging.info(f'Loaded dataset from {dataset_path} with {len(self.dataset)} samples.')

        self.log_config(vars(self))

    def run(
        self,
    ) -> list:
        """Evaluate the model on the dataset.

        Returns
        -------
        list
            List of messages generated by the model for each input in the dataset.

        """
        all_messages = []
        preds = []
        corrects = []
        errors = []
        i = 1

        runner = Runner(
            model_name=self.model_name,
            toolbox=self.toolbox,
            n_shots=self.n_shots,
            debug=self.debug,
        )

        for _, row in self.dataset.iterrows():
            # Reset runner for each question
            runner.reset()

            # os.system('cls' if os.name == 'nt' else 'clear')

            logging.info(f'✨ Processing question {i}/{len(self.dataset)}')

            # Log question metadata
            logging.info('🔎 Question Metadata')
            self.log_question_info(row)

            # Run the model on the question
            messages = runner.loop(row['question'])

            # Extract gold answer and answer_format
            gold_answer = row['answer']
            answer_format = row['answer_format']

            # Use Runner for evaluation and logging
            match_result = runner.match_results(messages, gold_answer, answer_format)
            correct, error = match_result
            corrects.append(correct if correct is not None else False)
            errors.append(error)

            all_messages.append(runner.format_messages(messages))

            # Use Matcher.extract_final_answer to get the model prediction
            pred = runner.matcher.extract_final_answer(messages)
            preds.append(pred)

            i += 1

        self.dataset['messages'] = all_messages
        self.dataset['pred'] = preds
        self.dataset['correct'] = corrects
        self.dataset['error'] = errors

        if self.save:
            model_name = str(self.model_name).split('/')[-1]
            output_path = Path('eval', 'runs', f'{model_name}_{self.split}.jsonl')
            output_path.parent.mkdir(parents=True, exist_ok=True)
            self.dataset.to_json(output_path, orient='records', lines=True)
            logging.info(f'Saved evaluation results to {output_path}')

        return all_messages

    def log_config(
        self,
        config: dict,
    ) -> None:
        """Log the configuration in a formatted way.

        Parameters
        ----------
        config : dict
            Configuration dictionary to log.

        """
        key_width = max(len(str(k)) for k in config)
        logging.info('Model Config')
        for k, v in config.items():
            if k == 'dataset':
                continue
            arrow = '-' * (key_width + 1 - len(str(k))) + '>'
            logging.info(f"⚙️ '{k}' {arrow} {v!r}")

    def log_question_info(
        self,
        row: pd.Series,
    ) -> None:
        """Log metadata in a formatted table.

        Parameters
        ----------
        metadata : dict
            Metadata dictionary to log.

        """
        keys = ['question_template', 'slot_values', 'answerable', 'answer_format', 'data_availability']
        key_width = max(len(str(k)) for k in keys)
        for k in keys:
            if k == 'slot_values':
                for sk, sv in row.get(k, {}).items():
                    # Arrow line replaces padding: key + ('-' * (key_width - len(key))) + '>'
                    arrow = '-' * (key_width + 1 - len(str(sk))) + '>'
                    logging.info(f"🔑 '{sk}' {arrow} {sv!r}")
            else:
                v = row.get(k)
                # Arrow line replaces padding: key + ('-' * (key_width - len(key))) + '>'
                arrow = '-' * (key_width + 1 - len(str(k))) + '>'
                logging.info(f"🔑 '{k}' {arrow} {v!r}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Evaluate a transformer model.')
    parser.add_argument(
        '--model-name',
        type=str,
        default='/public/hf/models/meta-llama/Meta-Llama-3.1-8B-Instruct/',
        help='Path or name of the transformer model.',
    )
    parser.add_argument(
        '--split',
        type=str,
        default='answerable-full',
        help='Dataset split to use (e.g., "answerable-full", "unanswerable-partial", etc.).',
    )
    parser.add_argument(
        '--num-samples',
        type=int,
        default=-1,
        help='Number of samples to evaluate. Use -1 for all samples.',
    )
    parser.add_argument(
        '--toolbox',
        type=str,
        choices=['all', 'arithmetic', 'data', 'none'],
        default='all',
        help='Toolbox to use for the evaluation. Can be "all", "arithmetic", "data", or "none".',
    )
    parser.add_argument(
        '--save',
        action='store_true',
        help='Whether to save the evaluation results.',
    )
    parser.add_argument(
        '--n-shots',
        type=int,
        default=0,
        help='Number of n-shot tool call examples to prepend to the prompt.',
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='If set, the loop will wait for user input after each message.',
    )
    args = parser.parse_args()

    evaluator = FrankensteinEvaluator(
        model_name=args.model_name,
        toolbox=args.toolbox,
        save=args.save,
        num_samples=args.num_samples,
        split=args.split,
        n_shots=args.n_shots,
        debug=args.debug,  # Pass debug argument
    )
    evaluator.args = args  # Attach args for logging

    evaluator.run()
